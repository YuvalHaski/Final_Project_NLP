{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-18T17:06:22.900634Z",
     "start_time": "2024-08-18T17:06:19.205193Z"
    }
   },
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from itertools import combinations\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T17:06:24.847697Z",
     "start_time": "2024-08-18T17:06:22.901641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('nlp_papers.csv')\n",
    "df['abstract'] = df['abstract'].fillna('')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n",
    "    # Remove stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "df['processed_text'] = df['abstract'].apply(preprocess_text)\n",
    "sentences = df['processed_text'].apply(lambda x: x.split()).tolist()"
   ],
   "id": "8d97ba68807272e6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 1: Train Word2Vec models with different vector sizes",
   "id": "ac35aca02b7ef2ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T17:06:27.727194Z",
     "start_time": "2024-08-18T17:06:24.848704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_sizes = [50, 100, 150, 200, 250, 300]\n",
    "window_size = 5\n",
    "models = {}\n",
    "\n",
    "for vector_size in vector_sizes:\n",
    "    model = Word2Vec(sentences=sentences, vector_size=vector_size, window=window_size, min_count=5, workers=4)\n",
    "    models[vector_size] = model\n",
    "    model.save(f'word2vec_vector_size_{vector_size}.model')"
   ],
   "id": "47ca4bc802ba5829",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 2: Define a function to calculate cosine similarity",
   "id": "46a36cdce2ed691d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T17:06:27.730789Z",
     "start_time": "2024-08-18T17:06:27.727194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ],
   "id": "d1aa7285cf3d20b2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 3: For each model, find the top 10 word pairs with the highest similarity",
   "id": "f280ec2c5a357980"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T17:14:58.503844Z",
     "start_time": "2024-08-18T17:06:27.732059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_pairs_results = {}\n",
    "\n",
    "for vector_size, model in models.items():\n",
    "    word_pairs = list(combinations(model.wv.index_to_key, 2))  # Generate all possible word pairs\n",
    "    similarities = []\n",
    "\n",
    "    for word1, word2 in word_pairs:\n",
    "        vec1 = model.wv[word1]\n",
    "        vec2 = model.wv[word2]\n",
    "        similarity = cosine_similarity(vec1, vec2)\n",
    "        similarities.append((word1, word2, similarity))\n",
    "\n",
    "    # Sort the pairs by similarity and get the top 20\n",
    "    top_pairs = sorted(similarities, key=lambda x: x[2], reverse=True)[:20]\n",
    "    top_pairs_results[vector_size] = top_pairs"
   ],
   "id": "1d471e427cc25c77",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 4: Display the results",
   "id": "9783427ed74ef016"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T17:14:58.511571Z",
     "start_time": "2024-08-18T17:14:58.504852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for vector_size, top_pairs in top_pairs_results.items():\n",
    "    print(f\"\\nTop 20 word pairs with highest similarity for vector size {vector_size} and window size {window_size}:\")\n",
    "    for word1, word2, similarity in top_pairs:\n",
    "        print(f\"{word1} - {word2}: {similarity:.4f}\")"
   ],
   "id": "fe1974c50b65acb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 word pairs with highest similarity for vector size 50 and window size 5:\n",
      "influence - formation: 0.9989\n",
      "profile - policy: 0.9988\n",
      "appropriate - account: 0.9988\n",
      "subject - taken: 0.9988\n",
      "investigation - action: 0.9987\n",
      "region - strain: 0.9987\n",
      "element - involved: 0.9987\n",
      "receptor - elegans: 0.9987\n",
      "allows - theory: 0.9987\n",
      "largest - taken: 0.9987\n",
      "ratio - interval: 0.9987\n",
      "without - transcript: 0.9987\n",
      "list - map: 0.9987\n",
      "form - collection: 0.9987\n",
      "certain - video: 0.9987\n",
      "involved - regulation: 0.9987\n",
      "movement - climate: 0.9987\n",
      "receptor - sleep: 0.9987\n",
      "element - regulation: 0.9987\n",
      "increased - change: 0.9987\n",
      "\n",
      "Top 20 word pairs with highest similarity for vector size 100 and window size 5:\n",
      "response - cell: 0.9995\n",
      "behavior - site: 0.9993\n",
      "regarding - china: 0.9993\n",
      "subject - following: 0.9993\n",
      "policy - nm: 0.9993\n",
      "appropriate - compound: 0.9993\n",
      "subject - leading: 0.9993\n",
      "especially - despite: 0.9993\n",
      "stability - profile: 0.9992\n",
      "region - return: 0.9992\n",
      "company - regulation: 0.9992\n",
      "world - society: 0.9992\n",
      "behavior - pathway: 0.9992\n",
      "cycle - pump: 0.9992\n",
      "least - respect: 0.9992\n",
      "china - society: 0.9992\n",
      "index - predicted: 0.9992\n",
      "cause - likely: 0.9992\n",
      "response - change: 0.9991\n",
      "company - usage: 0.9991\n",
      "\n",
      "Top 20 word pairs with highest similarity for vector size 150 and window size 5:\n",
      "element - regulation: 0.9996\n",
      "required - must: 0.9995\n",
      "chemical - stream: 0.9995\n",
      "possible - investigation: 0.9995\n",
      "subject - largest: 0.9995\n",
      "subject - sleep: 0.9995\n",
      "designed - specifically: 0.9995\n",
      "concern - might: 0.9995\n",
      "cause - country: 0.9995\n",
      "surgery - woman: 0.9995\n",
      "minlp - formulated: 0.9994\n",
      "cycle - expected: 0.9994\n",
      "practice - whether: 0.9994\n",
      "country - affected: 0.9994\n",
      "filter - rather: 0.9994\n",
      "price - load: 0.9994\n",
      "therefore - manufacturing: 0.9994\n",
      "filter - upon: 0.9994\n",
      "importance - bias: 0.9994\n",
      "region - policy: 0.9994\n",
      "\n",
      "Top 20 word pairs with highest similarity for vector size 200 and window size 5:\n",
      "region - surface: 0.9997\n",
      "subject - appropriate: 0.9997\n",
      "behavior - growth: 0.9997\n",
      "difference - less: 0.9996\n",
      "company - regulation: 0.9996\n",
      "food - leading: 0.9996\n",
      "region - nm: 0.9996\n",
      "u - filter: 0.9996\n",
      "primary - ct: 0.9996\n",
      "appropriate - taken: 0.9995\n",
      "country - china: 0.9995\n",
      "country - plan: 0.9995\n",
      "profile - stream: 0.9995\n",
      "country - serious: 0.9995\n",
      "developing - device: 0.9995\n",
      "expected - nitrogen: 0.9995\n",
      "regulation - action: 0.9995\n",
      "defined - following: 0.9995\n",
      "company - sector: 0.9995\n",
      "certain - upon: 0.9995\n",
      "\n",
      "Top 20 word pairs with highest similarity for vector size 250 and window size 5:\n",
      "de - la: 0.9998\n",
      "even - filter: 0.9997\n",
      "cycle - expected: 0.9997\n",
      "provided - experience: 0.9997\n",
      "ratio - interval: 0.9997\n",
      "global - uncertainty: 0.9997\n",
      "activity - action: 0.9997\n",
      "food - taken: 0.9996\n",
      "maximum - concentration: 0.9996\n",
      "analyze - focused: 0.9996\n",
      "concentration - pressure: 0.9996\n",
      "global - considering: 0.9996\n",
      "behavior - regulation: 0.9996\n",
      "injury - woman: 0.9996\n",
      "involved - action: 0.9996\n",
      "cycle - respect: 0.9996\n",
      "activity - urban: 0.9996\n",
      "concern - investigation: 0.9996\n",
      "environment - account: 0.9996\n",
      "region - electric: 0.9996\n",
      "\n",
      "Top 20 word pairs with highest similarity for vector size 300 and window size 5:\n",
      "major - concern: 0.9997\n",
      "primary - ct: 0.9997\n",
      "appropriate - caused: 0.9997\n",
      "regulation - experience: 0.9997\n",
      "whereas - detected: 0.9997\n",
      "profile - region: 0.9997\n",
      "cycle - electric: 0.9997\n",
      "food - sleep: 0.9997\n",
      "behavior - food: 0.9997\n",
      "investigation - action: 0.9997\n",
      "food - acid: 0.9997\n",
      "produced - surface: 0.9997\n",
      "regarding - experience: 0.9997\n",
      "candidate - attribute: 0.9997\n",
      "food - compound: 0.9997\n",
      "behavior - customer: 0.9997\n",
      "characteristic - least: 0.9997\n",
      "expected - electric: 0.9997\n",
      "receptor - taken: 0.9997\n",
      "possible - hand: 0.9997\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Comparison of Results Based on Changing Vector Size",
   "id": "efe9b431ca091357"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Objective: The goal of this comparison is to evaluate the quality of results obtained from a Word2Vec model when altering the vector size, one of the key hyperparameters, while keeping the window size constant. This analysis will help determine the impact of vector size on the semantic relationships captured by the model.\n",
    "\n",
    "## Summary of Results:\n",
    "Vector Size 50:\n",
    "\n",
    "* Key Pairs: \"influence - formation,\" \"profile - policy,\" \"appropriate - account\"\n",
    "* Observations: The pairs are reasonable but seem somewhat generic, lacking in nuanced relationships that might be more evident with larger vector sizes.\n",
    "\n",
    "\n",
    "Vector Size 100:\n",
    "\n",
    "* Key Pairs: \"response - cell,\" \"behavior - site,\" \"regarding - china\"\n",
    "* Observations: The pairs start to capture more specific relationships, such as \"response - cell,\" indicating a move towards more meaningful word pairings.\n",
    "\n",
    "Vector Size 150:\n",
    "\n",
    "* Key Pairs: \"element - regulation,\" \"required - must,\" \"chemical - stream\"\n",
    "* Observations: There's a noticeable improvement in the specificity and relevance of word pairs. Pairs like \"required - must\" suggest the model is capturing logical relationships.\n",
    " \n",
    "Vector Size 200:\n",
    "\n",
    "* Key Pairs: \"region - surface,\" \"subject - appropriate,\" \"behavior - growth\"\n",
    "* Observations: The model continues to improve, with pairs reflecting stronger semantic links, particularly in technical contexts (e.g., \"region - surface,\" \"behavior - growth\").\n",
    " \n",
    "Vector Size 250:\n",
    "\n",
    "* Key Pairs: \"de - la,\" \"cycle - expected,\" \"global - uncertainty\"\n",
    "* Observations: The model captures both generic and specific relationships. However, some pairs like \"de - la\" might indicate that the model is picking up on language artifacts rather than meaningful semantic relationships.\n",
    " \n",
    "Vector Size 300:\n",
    "\n",
    "* Key Pairs: \"major - concern,\" \"primary - ct,\" \"appropriate - caused\"\n",
    "* Observations: The model exhibits high specificity with strong contextual relevance, capturing pairs like \"major - concern\" and \"behavior - customer.\" However, some pairs like \"de - la\" from vector size 250 indicate possible overfitting or noise in the model."
   ],
   "id": "17e9a9edaef4bdbc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analysis of Results",
   "id": "1516c886071d633e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Vector Size 50: The model with vector size 50 captures basic relationships but lacks depth and specificity. The pairs are somewhat generic and do not capture nuanced meanings effectively.\n",
    "\n",
    "Vector Size 100: Shows an improvement over size 50, with more relevant pairs that indicate a better understanding of context. This size strikes a balance between generalization and specificity.\n",
    "\n",
    "Vector Size 150: Further improvement is seen with vector size 150, where the model begins to capture more complex relationships and logical pairings, making it a strong candidate for capturing nuanced meanings.\n",
    "\n",
    "Vector Size 200: Vector size 200 offers highly specific and contextually relevant word pairs, especially in technical or scientific contexts, making it a strong option for domains requiring detailed semantic understanding.\n",
    "\n",
    "Vector Size 250: While still strong, the results for vector size 250 show some signs of overfitting or picking up on irrelevant language artifacts, as seen in pairs like \"de - la.\" This might indicate that the model is becoming too specialized.\n",
    "\n",
    "Vector Size 300: This size provides very high specificity and relevance, capturing nuanced semantic relationships, but with the risk of overfitting or introducing noise, as seen in vector size 250."
   ],
   "id": "110a7199031afec3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Conclusion",
   "id": "615310faba787873"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Best Vector Size: Vector size 150 and vector size 200 both offer a good balance of specificity and relevance, making them the best choices depending on the domain and context. Vector size 150 might be more versatile, while vector size 200 is excellent for highly technical or detailed text analysis.\n",
    "\n",
    "Trade-offs: Smaller vector sizes (like 50 and 100) might miss out on capturing deeper semantic relationships, while larger sizes (250 and 300) could introduce noise or overfit to specific contexts."
   ],
   "id": "a74d5fc83277687f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
