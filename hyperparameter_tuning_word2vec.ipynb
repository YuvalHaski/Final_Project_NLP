{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-18T17:06:22.900634Z",
     "start_time": "2024-08-18T17:06:19.205193Z"
    }
   },
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from itertools import combinations\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T17:06:24.847697Z",
     "start_time": "2024-08-18T17:06:22.901641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('nlp_papers.csv')\n",
    "df['abstract'] = df['abstract'].fillna('')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n",
    "    # Remove stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "df['processed_text'] = df['abstract'].apply(preprocess_text)\n",
    "sentences = df['processed_text'].apply(lambda x: x.split()).tolist()"
   ],
   "id": "8d97ba68807272e6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 1: Train Word2Vec models with different vector sizes",
   "id": "ac35aca02b7ef2ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T17:06:27.727194Z",
     "start_time": "2024-08-18T17:06:24.848704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vector_sizes = [50, 100, 150, 200, 250, 300]\n",
    "window_size = 5\n",
    "models = {}\n",
    "\n",
    "for vector_size in vector_sizes:\n",
    "    model = Word2Vec(sentences=sentences, vector_size=vector_size, window=window_size, min_count=5, workers=4)\n",
    "    models[vector_size] = model\n",
    "    model.save(f'word2vec_vector_size_{vector_size}.model')"
   ],
   "id": "47ca4bc802ba5829",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 2: Define a function to calculate cosine similarity",
   "id": "46a36cdce2ed691d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T17:06:27.730789Z",
     "start_time": "2024-08-18T17:06:27.727194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ],
   "id": "d1aa7285cf3d20b2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 3: For each model, find the top 10 word pairs with the highest similarity",
   "id": "f280ec2c5a357980"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-18T17:06:27.732059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_pairs_results = {}\n",
    "\n",
    "for vector_size, model in models.items():\n",
    "    word_pairs = list(combinations(model.wv.index_to_key, 2))  # Generate all possible word pairs\n",
    "    similarities = []\n",
    "\n",
    "    for word1, word2 in word_pairs:\n",
    "        vec1 = model.wv[word1]\n",
    "        vec2 = model.wv[word2]\n",
    "        similarity = cosine_similarity(vec1, vec2)\n",
    "        similarities.append((word1, word2, similarity))\n",
    "\n",
    "    # Sort the pairs by similarity and get the top 20\n",
    "    top_pairs = sorted(similarities, key=lambda x: x[2], reverse=True)[:20]\n",
    "    top_pairs_results[vector_size] = top_pairs"
   ],
   "id": "1d471e427cc25c77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 4: Display the results",
   "id": "9783427ed74ef016"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "for vector_size, top_pairs in top_pairs_results.items():\n",
    "    print(f\"\\nTop 20 word pairs with highest similarity for vector size {vector_size} and window size {window_size}:\")\n",
    "    for word1, word2, similarity in top_pairs:\n",
    "        print(f\"{word1} - {word2}: {similarity:.4f}\")"
   ],
   "id": "fe1974c50b65acb3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
